{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from hashlib import sha256\n",
    "from itertools import combinations\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import scipy\n",
    "from scipy.stats import skew\n",
    "from distfit import distfit\n",
    "from statsmodels.stats.power import tt_ind_solve_power\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SOj37jwaZ4Z-",
    "outputId": "611af048-d769-4305-f7f4-1e0d44ca3058",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data0 = pd.read_csv(\"rekko_split_v1_1_transitioned.csv\")\n",
    "var_names = ['test_user_id', 'tag', 'watched_time', 'total_revenue', 'selects_from_rails', 'twt_rails', 'twt_rekko_rails']\n",
    "data0[\"watched_time\"] = data0[\"watched_time\"]/3600\n",
    "data0[\"twt_rails\"] = data0[\"twt_rails\"]/3600\n",
    "data0[\"twt_rekko_rails\"] = data0[\"twt_rekko_rails\"]/3600\n",
    "data0[\"total_revenue\"] = data0[\"tvod_revenue\"] + data0[\"svod_revenue\"]\n",
    "data0[\"total_purchases\"] = data0[\"tvod_purchases\"] + data0[\"svod_purchases\"]\n",
    "data1 = data0[var_names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZcuY4V4xZ86Y",
    "outputId": "a48c2a57-451c-4ba0-a044-97680c8a416a"
   },
   "outputs": [],
   "source": [
    "data1_test1 = data1.loc[data1['tag'] == 'test1']\n",
    "data1_control = data1.loc[data1['tag'] == 'control']\n",
    "data1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing very large outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude_cols = ['test_user_id', 'tag']\n",
    "for col in data1_test1.columns:\n",
    "    if col in exclude_cols:\n",
    "        continue\n",
    "    threshold1 = data1_test1[col].quantile(0.99999)\n",
    "    above_threshold1 = data1_test1[data1_test1[col] <= threshold1]\n",
    "    data1_test1 = above_threshold1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in data1_control.columns:\n",
    "    if col in exclude_cols:\n",
    "        continue\n",
    "    threshold3 = data1_control[col].quantile(0.99999)\n",
    "    above_threshold3 = data1_control[data1_control[col] <= threshold3]\n",
    "    data1_control = above_threshold3.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd.concat([data1_test1, data1_control], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_names = ['watched_time', 'twt_rails', 'twt_rekko_rails', 'total_revenue', 'selects_from_rails']\n",
    "\n",
    "stats2 = data1_control[metric_names].describe(percentiles=[.01,.05,.25,.50,.75, .95,.99])\n",
    "#stats2 = data1_test1[metric_names].describe(percentiles=[.01,.05,.25,.50,.75, .95,.99])\n",
    "\n",
    "skewness = data1_control[metric_names].apply(skew)\n",
    "\n",
    "stats2.loc['skewness'] = skewness\n",
    "\n",
    "\n",
    "num_zeros = (data1_control[metric_names] == 0).sum()\n",
    "\n",
    "# Calculate the total number of rows\n",
    "total_rows = data1_control[metric_names].shape[0]\n",
    "\n",
    "# Calculate the percentage of zeros\n",
    "percentage_zeros = (num_zeros / total_rows) * 100\n",
    "\n",
    "stats2.loc['zeros'] = percentage_zeros\n",
    "stats2.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60fMxFify2YF"
   },
   "source": [
    "# Metrics distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "n-jEukJBy1L9",
    "outputId": "2ff6dbd2-d2ed-4976-9d64-67463000e24c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# Function to plot distribution for a given metric and data type\n",
    "def plot_distribution(metric, data_type):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.suptitle(f\"Distribution of {metric} - {data_type}\", y=1.05)\n",
    "    #plt.subplots_adjust(top=0.85)\n",
    "\n",
    "    data = None\n",
    "    color = None\n",
    "\n",
    "    if data_type == 'test1':\n",
    "        data = data1_test1\n",
    "        color = 'red'\n",
    "    elif data_type == 'control':\n",
    "        data = data1_control\n",
    "        color = 'blue'\n",
    "    else:\n",
    "        print(\"Invalid data type. Please choose from 'test1', 'test2', or 'control'.\")\n",
    "        return\n",
    "    \n",
    "    #plt.subplot(1, 1, 1)\n",
    "    positive_data = [x for x in data[metric] if x > 0]\n",
    "    sns.histplot(positive_data,kde=\"True\", label=data_type, color=color, bins = 100)\n",
    "    plt.xlim()\n",
    "    plt.xlabel(\"Values\")\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "# Input the metric of your choice\n",
    "metric = input(\"Enter the metric to display distribution: \")\n",
    "\n",
    "plot_distribution(metric, 'test1')\n",
    "plot_distribution(metric, 'control')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ufkQyLVsy6A1"
   },
   "source": [
    "# Deltas distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate deltas for a specific metric\n",
    "def calculate_delta(metric_name, test_data, control_data):\n",
    "    test_data = test_data.sort_values(ascending=False).reset_index(drop=True)\n",
    "    control_data = control_data.sort_values(ascending=False).reset_index(drop=True)\n",
    "\n",
    "    min_len = min(len(test_data), len(control_data))\n",
    "    delta = []\n",
    "    for i in range(min_len):\n",
    "        if test_data[i] == 0 and control_data[i] == 0:\n",
    "            continue  # skip subtraction if both values are zero\n",
    "        else:\n",
    "            delta.append(test_data[i] - control_data[i])\n",
    "    delta = pd.Series(delta).sort_values()\n",
    "    return delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "5ekE-QPAooLa",
    "outputId": "3e681d88-2690-486e-b1e5-a7b9e7e96417",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Function to display delta for a specific metric\n",
    "def display_delta(metric_name, delta):\n",
    "    print(\"Delta for\", metric_name, \":\", delta)\n",
    "\n",
    "# Function to display delta histogram for a specific metric\n",
    "def display_delta_histogram(metric_name, delta):\n",
    "    \n",
    "        range_start, range_end = np.min(delta), np.max(delta)\n",
    "        title = \"Delta Distribution for \" + metric_name\n",
    "\n",
    "        sns.histplot(delta, stat='density', color='darkblue', kde=\"True\",bins=100)\n",
    "        plt.title(title)\n",
    "        plt.xlabel(\"Delta Value\")\n",
    "        plt.ylabel(\"Density\")\n",
    "        plt.xlim(range_start, range_end)  # Set x-axis range\n",
    "        plt.show()\n",
    "\n",
    "# Input the metric name for which you want to display the delta\n",
    "metric_name_input = input(\"Enter the metric name for which you want to display the delta and histogram: \")\n",
    "\n",
    "# Check if the input metric name is valid\n",
    "if metric_name_input in metric_names:\n",
    "    test1_data = data1_test1[metric_name_input]\n",
    "    control_data = data1_control[metric_name_input]\n",
    "    \n",
    "\n",
    "    # Calculate deltas\n",
    "    delta_test1 = calculate_delta(metric_name_input, test1_data, control_data)\n",
    "\n",
    "    # Display deltas for the chosen metric\n",
    "    display_delta(metric_name_input, delta_test1)\n",
    "\n",
    "    # Display delta histogram for the chosen metric\n",
    "    display_delta_histogram(metric_name_input, delta_test1)\n",
    "else:\n",
    "    print(\"Invalid metric name. Please choose from the following metric names:\")\n",
    "    print(metric_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3yqX_xr2SZi"
   },
   "source": [
    "## Deltas distributions types using distfit package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from distribution_fit import fit_distribution\n",
    "def fit_distributions(metric_names, test_data, control_data, test_data_name, verbose=False, num_processes=None):\n",
    "    deltas = []\n",
    "    for metric_name in metric_names:\n",
    "        delta = calculate_delta(metric_name, test_data[metric_name], control_data[metric_name])\n",
    "        delta = delta[delta>0]\n",
    "        deltas.append((metric_name, delta))\n",
    "    with Pool(num_processes) as pool:\n",
    "        results = pool.starmap(fit_distribution, deltas)\n",
    "    result_table = pd.DataFrame(results, columns=['Metric', 'Best Fitted Distribution', 'Score', 'Loc', 'Scale', 'Params'])\n",
    "    return result_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "result_table_delta = fit_distributions(metric_names, data1_test1, data1_control, \"data1_test1\", verbose=True, num_processes=6)\n",
    "result_table_delta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Corectness and sensitivity (A/A & A/B testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from salty import run_salt\n",
    "def check_test(data, nominator, split_count, method, effects, denominator=None, N=10, split_by='userid', num_processes=None):\n",
    "    \n",
    "    p_values = []\n",
    "    power = []\n",
    "\n",
    "    df = data.copy()\n",
    "\n",
    "    with Pool(num_processes) as pool:\n",
    "        results = []\n",
    "\n",
    "        for salt in tqdm(range(N)):\n",
    "            results.append(pool.apply_async(run_salt, (salt, df, split_count, nominator, method, effects, denominator, split_by)))\n",
    "\n",
    "        for result in tqdm(results):\n",
    "            p_values_per_combo, power_per_combo = result.get()\n",
    "            p_values.append(p_values_per_combo)\n",
    "            power.append(power_per_combo)\n",
    "    return np.array(p_values), np.array(power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "effects = []\n",
    "for i in [1,2,3,4,5,10,12.5,15]:\n",
    "    effect = [0, i/100, i/100]\n",
    "    effects.append(effect)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for metric_name in metric_names:\n",
    "    metric_results = []\n",
    "    for effect_set in effects:\n",
    "        p_values, power = check_test(data1, metric_name, 2, scipy.stats.ttest_ind, effect_set, denominator=None, N=1000, split_by='test_user_id', num_processes=12)\n",
    "        fpr1 = 100 * sum(np.array(p_values[:,0]) < 0.05) / len(p_values[:,0])\n",
    "        sens1 = 100 * sum(np.array(power[:,0]) < 0.05) / len(power[:,0])\n",
    "        effect_sizes = ';'.join([str(effect_size) for effect_size in effect_set])\n",
    "        metric_results.append([f\"{fpr1:.2f}%\", f\"{sens1:.2f}%\"])\n",
    "\n",
    "    all_results.append([metric_name] + metric_results)\n",
    "\n",
    "table = pd.DataFrame(all_results, columns=['Metric'] + [f\"({';'.join([str(effect_size) for effect_size in effect])})\" for effect in effects])\n",
    "table.set_index('Metric', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iKaD_nc9RHy"
   },
   "source": [
    "# MDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "xR2eGWva9Swi"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "FIRST_TYPE_ERROR = 0.05\n",
    "SECOND_TYPE_ERROR = 0.2\n",
    "ddof = 1\n",
    "\n",
    "\n",
    "def get_parameter_size(\n",
    "    nominator,\n",
    "    parameter,\n",
    "    split_count=2,\n",
    "    effect=0.01,\n",
    "    denominator=None,\n",
    "    ratio=1.0,\n",
    "    alpha=FIRST_TYPE_ERROR,\n",
    "    beta=SECOND_TYPE_ERROR,\n",
    "    alternative=\"two-sided\",\n",
    "    ddof=1,\n",
    "):\n",
    "    if denominator is None:\n",
    "        denominator = np.ones(len(nominator))\n",
    "\n",
    "    pairing_cnt = split_count * (split_count - 1) / 2\n",
    "    alpha = alpha / pairing_cnt\n",
    "    \n",
    "    mean, std = np.mean(nominator), np.std(nominator / denominator, ddof=ddof)\n",
    "    nobs1 = len(nominator) / (1 + ratio)\n",
    "\n",
    "    if parameter == \"effect\":\n",
    "        return get_effect_size(mean, std, nobs1, ratio=ratio, alpha=alpha, beta=beta, alternative=alternative)\n",
    "    elif parameter == \"size\":\n",
    "        return get_sample_size(mean, std, effect, ratio=ratio, alpha=alpha, beta=beta, alternative=alternative)\n",
    "    elif parameter == \"power\":\n",
    "        return get_power_size(mean, std, nobs1, effect, ratio=ratio, alpha=alpha, alternative=alternative)\n",
    "    elif parameter == \"correctness\":\n",
    "        return get_correctness_size(mean, std, nobs1, effect, ratio=ratio, beta=beta, alternative=alternative)\n",
    "    else:\n",
    "        raise Exception('Uknown parameter. Use from available \"effect\", \"size\", \"power\", \"correctness\"')\n",
    "\n",
    "\n",
    "def get_effect_size(\n",
    "    mean, std, nobs1, alpha=FIRST_TYPE_ERROR, beta=SECOND_TYPE_ERROR, ratio=1.0, alternative=\"two-sided\"\n",
    "):\n",
    "    std_effect = tt_ind_solve_power(\n",
    "        effect_size=None, nobs1=nobs1, alpha=alpha, power=1 - beta, ratio=ratio, alternative=alternative\n",
    "    )\n",
    "    mde = std_effect * std / mean\n",
    "\n",
    "    return np.round(100.0 * mde, 2)\n",
    "\n",
    "\n",
    "def get_sample_size(\n",
    "    mean, std, effect, alpha=FIRST_TYPE_ERROR, beta=SECOND_TYPE_ERROR, ratio=1.0, alternative=\"two-sided\"\n",
    "):\n",
    "    std_effect = mean * effect / std\n",
    "\n",
    "    sample_size = tt_ind_solve_power(\n",
    "        effect_size=std_effect, nobs1=None, alpha=alpha, power=1 - beta, ratio=ratio, alternative=alternative\n",
    "    )\n",
    "\n",
    "    return int(sample_size * (1 + ratio))\n",
    "\n",
    "\n",
    "def get_power_size(mean, std, nobs1, effect, alpha=FIRST_TYPE_ERROR, ratio=1.0, alternative=\"two-sided\"):\n",
    "    std_effect = mean * effect / std\n",
    "\n",
    "    power_size = tt_ind_solve_power(\n",
    "        effect_size=std_effect, nobs1=nobs1, alpha=alpha, power=None, ratio=ratio, alternative=alternative\n",
    "    )\n",
    "\n",
    "    return np.round(100.0 * power_size, 2)\n",
    "\n",
    "\n",
    "def get_correctness_size(mean, std, nobs1, effect, beta=SECOND_TYPE_ERROR, ratio=1.0, alternative=\"two-sided\"):\n",
    "    std_effect = mean * effect / std\n",
    "\n",
    "    correctness_size = tt_ind_solve_power(\n",
    "        effect_size=std_effect, nobs1=nobs1, alpha=None, power=1 - beta, ratio=ratio, alternative=alternative\n",
    "    )\n",
    "\n",
    "    return np.round(100.0 * correctness_size, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_sizes(data, metric_names, split_count=2, denominator=None, ratio=1.0, alpha=0.05, beta=0.2, alternative=\"two-sided\", ddof=1):\n",
    "    effect_sizes = [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.125, 0.15, 0.20]\n",
    "    sample_sizes = {}\n",
    "\n",
    "    for effect in effect_sizes:\n",
    "        sample_sizes[effect] = {}\n",
    "        for metric_name in metric_names:\n",
    "            metric_data = data1_control[metric_name]\n",
    "            size = get_parameter_size(metric_data, \"size\", split_count, effect, denominator, ratio, alpha, beta, alternative, ddof)\n",
    "            sample_sizes[effect][metric_name] = size\n",
    "\n",
    "    return pd.DataFrame(sample_sizes)\n",
    "get_sample_sizes(data1_control, metric_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sample_sizes(data, metric_names, split_count=2, denominator=None, ratio=1.0, alpha=0.05, beta=0.2, alternative=\"two-sided\", ddof=1):\n",
    "    effect_sizes = [0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.125, 0.15, 0.20]\n",
    "    sample_sizes = {}\n",
    "    for metric_name in metric_names:\n",
    "        metric_data = data1_control[metric_name]\n",
    "        effect1 = get_parameter_size(metric_data, \"effect\", split_count, None, denominator, ratio, alpha, beta, alternative, ddof)\n",
    "        sample_sizes[metric_name] = [effect1]\n",
    "\n",
    "    return pd.DataFrame(sample_sizes)\n",
    "get_sample_sizes(data1_control, metric_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CUPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4 = pd.read_csv('similars_new_model_v2.csv')\n",
    "data3 = pd.read_csv('similars_new_model_v2_pred.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3 = data3[data3['has_completed_film']==1]\n",
    "data4 = data4[data4['has_completed_film']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cuped=pd.merge(data3, data4, on='test_user_id')\n",
    "data_cuped['twt_similar_after_watch_x'] = data_cuped['twt_similar_after_watch_x']/3600\n",
    "data_cuped['twt_similar_after_watch_y'] = data_cuped['twt_similar_after_watch_y']/3600\n",
    "\n",
    "data_cuped['twt_x'] = data_cuped['twt_x']/3600\n",
    "data_cuped['twt_y'] = data_cuped['twt_y']/3600"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression (Average Treatment Effect) before CUPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "np.mean(data_cuped.loc[data_cuped[\"tag_x\"]=='test1', 'twt_similar_after_watch_y']) - np.mean(data_cuped.loc[data_cuped[\"tag_x\"]=='control', 'twt_similar_after_watch_y'])\n",
    "print(smf.ols('twt_similar_after_watch_y ~ tag_x', data=data_cuped).fit().summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = smf.ols('twt_similar_after_watch_y ~ twt_similar_after_watch_x', data=data_cuped).fit().params[1]\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_cuped['twt_similar_after_watch_cuped'] = data_cuped['twt_similar_after_watch_y'] - theta * (data_cuped['twt_similar_after_watch_x'] - np.mean(data_cuped['twt_similar_after_watch_x']))\n",
    "data_cuped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression (Average Treatment Effect) after CUPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(smf.ols('twt_similar_after_watch_cuped ~ tag_x', data=data_cuped).fit().summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_cuped[\"twt_similar_after_watch_cuped\"].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correctness and sensitivity for CUPED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "metric_names = ['twt_similar_after_watch_cuped', 'twt_similar_after_watch_y']\n",
    "effects = []\n",
    "for i in [1,2,3,4,5]:\n",
    "    effect = [0, i/100]\n",
    "    effects.append(effect)\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for metric_name in metric_names:\n",
    "    metric_results = []\n",
    "    for effect_set in effects:\n",
    "        p_values, power = check_test(data_cuped, metric_name, 2, scipy.stats.ttest_ind, effect_set, denominator=None, N=1000, split_by='test_user_id', num_processes=12)\n",
    "        fpr1 = 100 * sum(np.array(p_values[:,0]) < 0.05) / len(p_values[:,0])\n",
    "        sens1 = 100 * sum(np.array(power[:,0]) < 0.05) / len(power[:,0])\n",
    "        effect_sizes = ';'.join([str(effect_size) for effect_size in effect_set])\n",
    "        metric_results.append([f\"{fpr1:.2f}%\", f\"{sens1:.2f}%\"])\n",
    "\n",
    "    all_results.append([metric_name] + metric_results)\n",
    "\n",
    "table = pd.DataFrame(all_results, columns=['Metric'] + [f\"({';'.join([str(effect_size) for effect_size in effect])})\" for effect in effects])\n",
    "table.set_index('Metric', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#table.to_csv('cuped_v2.csv')\n",
    "table"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
